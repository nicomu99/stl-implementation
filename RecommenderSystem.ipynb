{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns are the items\n",
    "#rows are the transactions\n",
    "A = np.array([[1, 0, 1, 0, 1],\n",
    "              [1, 0, 1, 1, 0],\n",
    "              [0, 0, 0, 1, 1],\n",
    "              [0, 0, 1, 2, 0],\n",
    "              [2, 0, 0, 0, 0]])\n",
    "orders = A\n",
    "min_support = 1 # it was 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping the sparse matrix to records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: map the items of the records to ids and sort each record (1 points)\n",
    "mapped_records = []\n",
    "# In the following tasks use the mapped records to compute the frequent itemsets.\n",
    "for record in orders:\n",
    "    mapped_record = []\n",
    "    for index, item in enumerate(record):\n",
    "        if(item > 0):\n",
    "            mapped_record.append(index)\n",
    "    mapped_record.sort()\n",
    "    mapped_records.append(mapped_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4]\n",
      "[0, 2, 3]\n",
      "[3, 4]\n",
      "[2, 3]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "for record in mapped_records:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apriori Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate the support of length-1 itemsets using Counter or defaultdict (1 points)\n",
    "l1_items = collections.Counter()\n",
    "for record in mapped_records:\n",
    "    l1_items.update(record)\n",
    "    \n",
    "# TODO: filter out the frequent length-1 itemsets with their support (1 point)\n",
    "frequent_l1_items = {}\n",
    "for item in l1_items:\n",
    "    support = l1_items[item]\n",
    "    if support >= min_support:\n",
    "        frequent_l1_items[(item,)] = support\n",
    "\n",
    "# Store all frequent itemsets (keys) with their support (value) in this dictionary.\n",
    "# Hint: Convert the itemsets to tuples or sets so that you can use them as keys.\n",
    "# TODO: save the length-1 frequent items and their supports to frequent_itemsets (1 points)\n",
    "frequent_itemsets = {}\n",
    "\n",
    "for item in frequent_l1_items:\n",
    "    frequent_itemsets[item] = frequent_l1_items[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the apriori_gen algorithm based on the lecture slides\n",
    "def apriori_gen(itemsets):\n",
    "    # TODO: generate candidates (4 points)\n",
    "    C_k = set()\n",
    "    for p in itemsets:\n",
    "        for q in itemsets:\n",
    "            if p[-1] < q[-1]:\n",
    "                C_k.add( p + (q[-1],) )\n",
    "        \n",
    "    # TODO: prune the candidates and return them (4 points)\n",
    "    def all_subsets_in_itemsets(x):\n",
    "        for subset in itertools.combinations(x, len(x) - 1):\n",
    "            if subset not in itemsets:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    return list(filter(all_subsets_in_itemsets, C_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement an algorithm to calculate the support of the given itemset (2 points)\n",
    "# You do not need to implement a Hash Tree for calculating the supports.\n",
    "def calculate_support(itemset):\n",
    "    if len(itemset) == 1:\n",
    "        try:\n",
    "            return frequent_l1_items[itemset]\n",
    "        except KeyError:\n",
    "            return 0\n",
    "        \n",
    "    support = 0\n",
    "    for record in mapped_records:\n",
    "        itemset_in_record = True\n",
    "        for item in itemset:\n",
    "            if item not in record:\n",
    "                itemset_in_record = False\n",
    "                break\n",
    "        if itemset_in_record:\n",
    "            support += 1\n",
    "    return support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set the initial frequent itemsets which needs to be used in the first iteration (1 point)\n",
    "# (It will be updated after each iteration.)\n",
    "frequent_n_itemsets = frequent_l1_items\n",
    "\n",
    "# TODO: set the correct loop condition until the Apriori algorithm should run (1 point)\n",
    "while len(frequent_n_itemsets) != 0:\n",
    "    candidates = apriori_gen(frequent_n_itemsets)\n",
    "    supports = map(calculate_support, candidates)\n",
    "\n",
    "    # TODO: filter out the frequent candidates (2 point)\n",
    "    frequent_candidates = {}\n",
    "    for candidate, support in zip(candidates, supports):\n",
    "        if support >= min_support:\n",
    "            frequent_candidates[candidate] = support\n",
    "\n",
    "    # TODO: add the frequent candidates to frequent_itemsets (1 point)\n",
    "    for item in frequent_candidates:\n",
    "        frequent_itemsets[item] = frequent_candidates[item]\n",
    "    \n",
    "    # replace the frequent_n_itemsets for the next iteration\n",
    "    frequent_n_itemsets = [itemset for itemset in frequent_candidates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 : (0,)\n",
      "\n",
      "3 : (2,)\n",
      "\n",
      "2 : (4,)\n",
      "\n",
      "3 : (3,)\n",
      "\n",
      "1 : (2, 4)\n",
      "\n",
      "1 : (0, 4)\n",
      "\n",
      "1 : (3, 4)\n",
      "\n",
      "1 : (0, 3)\n",
      "\n",
      "2 : (2, 3)\n",
      "\n",
      "2 : (0, 2)\n",
      "\n",
      "1 : (0, 2, 4)\n",
      "\n",
      "1 : (0, 2, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for itemset in frequent_itemsets:\n",
    "    support = frequent_itemsets[itemset]\n",
    "    print(f\"{support} : {itemset}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendation(itemset, recommendation_amount = 2):\n",
    "    if hasattr(itemset, '__len__'):\n",
    "        products = tuple(sorted(itemset))\n",
    "    else:\n",
    "        products = (itemset,)\n",
    "        \n",
    "    confidences = {}\n",
    "    max_support = frequent_itemsets[products]\n",
    "    for itemset in filter(\n",
    "            lambda x:  len(x) == len(products) + 1 and all([y in x for y in products]), frequent_itemsets\n",
    "        ):\n",
    "        #filtering the already added items\n",
    "        for item in filter(lambda x: x not in products, itemset):\n",
    "            #calculating the confidence for recommending this item and adding it to the confidence dict\n",
    "            confidences[item] = frequent_itemsets[itemset] / max_support\n",
    "            \n",
    "    #sorting the confidences dict from biggest confidence to lowest\n",
    "    confidences = sorted(confidences.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    #printing the recommendations\n",
    "    for i in range(min(recommendation_amount, len(confidences))):\n",
    "        index, confidence = confidences[i]\n",
    "        print(f\"\\\"{index}\\\" is recommended with {confidence * 100:.2f}% confidence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"4\" is recommended with 50.00% confidence.\n",
      "\"3\" is recommended with 50.00% confidence.\n"
     ]
    }
   ],
   "source": [
    "#Example\n",
    "itemset = (0,2)\n",
    "#itemset = 2\n",
    "\n",
    "recommendation_amount = 2\n",
    "\n",
    "get_recommendation(itemset, recommendation_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
