{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Task 1 - Apriori Algorithm for Recommender System (33 points)\n",
    "\n",
    "**Goal:** The aim of this programming assignment is to implement the Apriori algorithm and apply it to mine frequent itemsets for recommendation. You are required to implement the algorithm from scratch by using only native Python libraries and NumPy. For efficiency you will need to convert the items to ids and sort them.\n",
    "\n",
    "**Input:** The provided input file (`orders.txt`) contains 461 lists of items, which are orders by customers in an online retail shop. [1] Each line in the file corresponds to an order and represents a list of products a costumer bought. An example:\n",
    "\n",
    "*Alarm Clock Bakelite Green;Panda And Bunnies Sticker Sheet*\n",
    "\n",
    "In the example above, a customer ordered the products \"Alarm Clock Bakelite Green\" and \"Panda And Bunnies Sticker Sheet\".\n",
    "\n",
    "**Output:** Implement the Apriori algorithm and use it to mine frequent itemsets. Set the relative minimum support to 0.025 and run the algorithm on the 461 orders of retail shop products. In other words, you need to extract all the itemsets that have an absolute support larger or equal to 12.\n",
    "\n",
    "[1] The dataset is a modified version of the france dataset from pycaret. (https://github.com/pycaret/pycaret/blob/master/datasets/france.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: uncomment the packages you used, please do not import additional non-native packages\n",
    "# You may change the imports to the following format: from [package] import [class, method, etc.]\n",
    "\n",
    "import collections\n",
    "import itertools\n",
    "#import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1.1 Loading the data and preprocessing (3 points)\n",
    "**Task:** Solve the tasks explained in the TODOs and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: read the data from the input file /data/orders.txt (1 points)\n",
    "\n",
    "orders_file = open(\"data/orders.txt\", \"r\")\n",
    "orders = []\n",
    "\n",
    "line = orders_file.readline()\n",
    "while line:\n",
    "    orders.append(line.strip().split(\";\"))\n",
    "    line = orders_file.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: determine the unique items and map the item to ids using enumerate (1 points)\n",
    "unique_items = []\n",
    "id_to_item = unique_items\n",
    "\n",
    "item_to_id = {}\n",
    "\n",
    "for record in orders:\n",
    "    for item in record:\n",
    "        if item not in unique_items:\n",
    "            unique_items.append(item)\n",
    "            \n",
    "for count, item in enumerate(unique_items):\n",
    "    item_to_id[item] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: map the items of the records to ids and sort each record (1 points)\n",
    "mapped_records = []\n",
    "# In the following tasks use the mapped records to compute the frequent itemsets.\n",
    "\n",
    "for record in orders:\n",
    "    mapped_record = []\n",
    "    for item in record:\n",
    "        mapped_record.append(item_to_id[item])\n",
    "    mapped_record.sort()\n",
    "    mapped_records.append(mapped_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1.2 Apriori algorithm (21 points)\n",
    "### A) Prune the infrequent items (3 points)\n",
    "**Task:** Solve the tasks explained in the TODOs and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: calculate the support of length-1 itemsets using Counter or defaultdict (1 points)\n",
    "l1_items = collections.Counter()\n",
    "\n",
    "for record in mapped_records:\n",
    "    l1_items.update(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-29T17:02:59.981935731Z",
     "start_time": "2023-07-29T17:02:59.981495655Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: filter out the frequent length-1 itemsets with their support (1 point)\n",
    "frequent_l1_items = {}\n",
    "\n",
    "for item in l1_items:\n",
    "    support = l1_items[item]\n",
    "    if support >= 12:\n",
    "        frequent_l1_items[(item,)] = support\n",
    "\n",
    "# Store all frequent itemsets (keys) with their support (value) in this dictionary.\n",
    "# Hint: Convert the itemsets to tuples or sets so that you can use them as keys.\n",
    "# TODO: save the length-1 frequent items and their supports to frequent_itemsets (1 points)\n",
    "frequent_itemsets = {}\n",
    "\n",
    "for item in frequent_l1_items:\n",
    "    frequent_itemsets[item] = frequent_l1_items[item]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### B) Determine the frequent n itemsets (15 points)\n",
    "**Task:** Solve the tasks explained in the TODOs and comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: implement the apriori_gen algorithm based on the lecture slides\n",
    "def apriori_gen(itemsets):\n",
    "    # TODO: generate candidates (4 points)\n",
    "    C_k = set()\n",
    "    for p in itemsets:\n",
    "        for q in itemsets:\n",
    "            if p[-1] < q[-1]:\n",
    "                C_k.add( p + (q[-1],) )\n",
    "        \n",
    "    # TODO: prune the candidates and return them (4 points)\n",
    "    def all_subsets_in_itemsets(x):\n",
    "        for subset in itertools.combinations(x, len(x) - 1):\n",
    "            if subset not in itemsets:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    return list(filter(all_subsets_in_itemsets, C_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: implement an algorithm to calculate the support of the given itemset (2 points)\n",
    "# You do not need to implement a Hash Tree for calculating the supports.\n",
    "def calculate_support(itemset):\n",
    "    if len(itemset) == 1:\n",
    "        try:\n",
    "            return frequent_l1_items[itemset]\n",
    "        except KeyError:\n",
    "            return 0\n",
    "        \n",
    "    support = 0\n",
    "    for record in mapped_records:\n",
    "        itemset_in_record = True\n",
    "        for item in itemset:\n",
    "            if item not in record:\n",
    "                itemset_in_record = False\n",
    "                break\n",
    "        if itemset_in_record:\n",
    "            support += 1\n",
    "    return support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: set the initial frequent itemsets which needs to be used in the first iteration (1 point)\n",
    "# (It will be updated after each iteration.)\n",
    "frequent_n_itemsets = frequent_l1_items\n",
    "\n",
    "# TODO: set the correct loop condition until the Apriori algorithm should run (1 point)\n",
    "while len(frequent_n_itemsets) != 0:\n",
    "    candidates = apriori_gen(frequent_n_itemsets)\n",
    "    supports = map(calculate_support, candidates)\n",
    "\n",
    "    # TODO: filter out the frequent candidates (2 point)\n",
    "    frequent_candidates = {}\n",
    "    for candidate, support in zip(candidates, supports):\n",
    "        if support >= 12:\n",
    "            frequent_candidates[candidate] = support\n",
    "\n",
    "    # TODO: add the frequent candidates to frequent_itemsets (1 point)\n",
    "    for item in frequent_candidates:\n",
    "        frequent_itemsets[item] = frequent_candidates[item]\n",
    "    \n",
    "    # replace the frequent_n_itemsets for the next iteration\n",
    "    frequent_n_itemsets = [itemset for itemset in frequent_candidates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### C) Save your results (3 points)\n",
    "\n",
    "**Task:** Save all the frequent itemsets along with their absolute supports into a text file named `patterns.txt` and place it in the root of your zip file. Every line corresponds to exactly one frequent itemset and should be in the following format:\n",
    "\n",
    "*support:product1;product2;product3;...*\n",
    "\n",
    "For example, suppose an itemset (Mini Paint Set Vintage;Picture Dominoes) has an absolute support 46, then the line corresponding to this frequent itemset in `patterns.txt` should be:\n",
    "\n",
    "*46:Mini Paint Set Vintage;Picture Dominoes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with open(\"patterns.txt\",\"w\") as patterns_file:\n",
    "    for itemset in frequent_itemsets:\n",
    "        support = frequent_itemsets[itemset]\n",
    "        products = ';'.join(map(lambda x: id_to_item[x], itemset))\n",
    "        patterns_file.write(f\"{support}:{products}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1.3 Recommendation (9 points)\n",
    "\n",
    "**Task:** Imagine you should recommend 2 other products to a customer who added \"Pack Of 6 Skull Paper Cups\" and \"Pack Of 20 Skull Paper Napkins\" to the cart. Based on the results of the Apriori algorithm, implement an algorithm that returns 2 products to display to the customer on the website by maximizing the confidence that the customer will buy the product. (6 points)\n",
    "\n",
    "**Report:** Explain your method (comments in code or summary) and display your recommendations with the confidence scores. (3 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Pack Of 6 Skull Paper Plates\" is recommended by 93.75% confidence.\n",
      "\"Set/20 Red Retrospot Paper Napkins\" is recommended by 81.25% confidence.\n"
     ]
    }
   ],
   "source": [
    "#inputs\n",
    "\n",
    "#list of product names\n",
    "products = [\"Pack Of 6 Skull Paper Cups\", \"Pack Of 20 Skull Paper Napkins\"]\n",
    "#amount of recommended products in the output\n",
    "recommendation_amount = 2\n",
    "\n",
    "#tuple after changing the product names to their ids and then sorting them\n",
    "products = tuple(sorted(map(lambda x: item_to_id[x], products)))\n",
    "\n",
    "#confidence is calculated by the support of the added products and \n",
    "# the potential recommendations divided by the support of the added products.\n",
    "#basically: conf = support(products + recommendation)/support(products)\n",
    "\n",
    "#dict to save recommendations with their confidence\n",
    "confidences = {}\n",
    "#calculating support(products)\n",
    "max_support = frequent_itemsets[products]\n",
    "\n",
    "#iteration over all frequent itemsets, that are supersets of and have one more product in the tuple then the added items,\n",
    "#exactly one more because of the anti-monotone property.\n",
    "for itemset in filter(\n",
    "        lambda x:  len(x) == len(products) + 1 and all([y in x for y in products]), frequent_itemsets\n",
    "    ):\n",
    "    #filtering the already added items\n",
    "    for item in filter(lambda x: x not in products, itemset):\n",
    "        #calculating the confidence for recommending this item and adding it to the confidence dict\n",
    "        confidences[item] = frequent_itemsets[itemset] / max_support\n",
    "        \n",
    "#sorting the confidences dict from biggest confidence to lowest\n",
    "confidences = sorted(confidences.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#printing the recommendations\n",
    "for i in range(min(recommendation_amount, len(confidences))):\n",
    "    id, confidence = confidences[i]\n",
    "    recommendation = id_to_item[id]\n",
    "    print(f\"\\\"{recommendation}\\\" is recommended by {confidence * 100:.2f}% confidence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
